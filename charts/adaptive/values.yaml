serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: serviceaccount.adaptive-ml.com

# Secrets configuration
# You can either provide values directly (and the chart will create secrets)
# or reference existing secrets that you provision yourself
secrets:
  # Optional: Reference existing secrets instead of creating new ones
  # Control Plane existing secret (must contain keys: dbUsername, dbPassword, dbHost, dbName, cookiesSecret, oidcProviders)
  existingControlPlaneSecret: ""
  # Harmony existing secret (must contain keys: modelRegistryUrl, sharedDirectoryUrl)
  existingHarmonySecret: ""
  # Redis existing secret (must contain key: redisUrl)
  existingRedisSecret: ""
  # S3 bucket for model registry
  modelRegistryUrl: s3://bucket-name/model_registry
  # Use same bucket as above and can use a different prefix
  sharedDirectoryUrl: s3://bucket-name/shared
  # Postgres database connection configuration
  db:
    username: username
    password: password
    host: db_address:5432  # Host and port (e.g., db_address:5432 or db_address)
    database: db_name
  # Secret used to sign cookies. Must be the same on all servers of a cluster and >= 64 chars
  cookiesSecret: change-me-secret-db40431e-c2fd-48a6-acd6-854232c2ed94-01dd4d01-dr7b-4315   # Must be >= 64 chars

  auth:
    oidc:
      providers:
        # Name of your OpenId provider displayed in the ui
        - name: "Google"
          # Key of your provider, the callback url will be '<rootUrl>/api/v1/auth/login/<key>/callback'
          key: "google"

          issuer_url: "https://accounts.google.com"   # openid connect issuer url

          client_id: "replace_client_id"   # client id

          client_secret: "replace_client_secret"   # client_secret, optional

          scopes: ["email", "profile"]   # scopes required for auth, requires email and profile

          # true if your provider supports pkce (recommended)
          pkce: true

          # if true, user account will be created if it does not exist
          allow_sign_up: true

          # if true, user email must be verified to log in to the application (default is true).
          # in some identity providers, this is not supported and should be opted out.
          require_email_verified: true

auth:
  # One of [admin, read-only, inference, annotator]
  default_role: admin
  session:
    # Set the secure flag for the session cookie: they are only valid on https and localhost
    # Should be true in prod - (use false if the app is accessed through insecure http)
    secure: true
    expiration_seconds: 518400   # 6 days
  # List of email addresses for admins; overrides default_role when these users are created
  admins: []

containerRegistry: <aws_account_id>.dkr.ecr.<region>.amazonaws.com   # Add the Adaptive Registry you have been granted access to

harmony:
  # Set to false to disable the harmony statefulset deployment (advanced mode only)
  enabled: true
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: harmony:latest   # Add the harmony image tag
    pullPolicy: Always

  # Default values inherited by all compute pools unless overridden
  # Should be equal to, or a divisor of the # of GPUs on each node
  # If you are not deploying on GPU, comment out the line
  gpusPerReplica: 8

  nodeSelector:   # Uncomment to deploy harmony on specify EKS node group or GKE node pool
    {}
    # eks.amazonaws.com/nodegroup: gpu-node-group-name
    # cloud.google.com/gke-accelerator: a2-ultragpu-4g

  # Default resource limits/requests - can be overridden per compute pool
  # Adjust to your deployment server specs and model size requirements
  # It is recommended to use as much RAM and CPU as your machine provides
  resources:
    limits:
      cpu: 30
      memory: 500Gi
    requests:
      cpu: 30
      memory: 500Gi

  # Size of /dev/shm (shared memory) volume for NCCL and other IPC
  # This should not be modified under normal circumstances
  shmSize: 32Gi

  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "adaptive"
    prometheus.io/path: /metrics
    prometheus.io/port: "50053"
  podLabels: {}
  extraEnvVars: {}
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"

  # Pod Disruption Budget for harmony compute pools
  # Ensures minimum availability during voluntary disruptions (node drains, upgrades)
  # NOTE: Specify either minAvailable OR maxUnavailable, not both
  # This applies to all compute pools
  pdb:
    enabled: false
    # minAvailable: 1    # Minimum number/percentage of pods that must remain available
    # maxUnavailable: 1  # Maximum number/percentage of pods that can be unavailable

  # List of compute pools - each creates a separate StatefulSet and headless Service
  # Pool-specific values override the defaults above
  computePools:
    - name: default
      replicas: 1
      # Optional pool-specific overrides (uncomment to use):
      # image:
      #   tag: harmony:latest  # Override image tag for this pool
      # gpusPerReplica: 8
      # capabilities: "TRAINING,INFERENCE,EVALUATION"  # Default capabilities
      # resources:
      #   limits:
      #     cpu: 30
      #     memory: 500Gi
      #   requests:
      #     cpu: 30
      #     memory: 500Gi
      # nodeSelector:  # Multiple node selector labels can be specified
      #   eks.amazonaws.com/nodegroup: gpu-node-group
      #   node.kubernetes.io/instance-type: p4d.24xlarge
      # tolerations: []
      # podAnnotations: {}
      # podLabels: {}
      # extraEnvVars: {}
      # shmSize: 32Gi

    # Example: Add additional pools for different workloads
    # - name: inference
    #   replicas: 2
    #   image:
    #     tag: harmony:inference-optimized  # Different image tag for inference pool
    #   gpusPerReplica: 4
    #   capabilities: "INFERENCE"
    #   resources:
    #     limits:
    #       cpu: 16
    #       memory: 128Gi
    #     requests:
    #       cpu: 16
    #       memory: 128Gi
    #   nodeSelector:
    #     cloud.google.com/gke-accelerator: nvidia-l4
    #     cloud.google.com/gke-spot: "true"
    #   tolerations:
    #     - key: "nvidia.com/gpu"
    #       operator: "Exists"
    #       effect: "NoSchedule"
    # - name: training
    #   replicas: 1
    #   gpusPerReplica: 8
    #   capabilities: "TRAINING,EVALUATION"
    #   nodeSelector:
    #     eks.amazonaws.com/nodegroup: training-nodes
    #     topology.kubernetes.io/zone: us-east-1a

controlPlane:
  sharedDirType: s3
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: control-plane:latest   # Add the control plane image tag
    pullPolicy: Always

  servicePort: 80   # Port where app will be exposed

  # Full url of the application as visible from a web browser. Important if you use SSO
  rootUrl: "http://localhost:9000"

  # rootUrl: "https://YOUR_URL"

  # Update the DB schema; defaults to True unless explictly False
  runDbMigrations: true

  annotations: {}
  podAnnotations:
    ingest-adaptive-logs: "true"
    prometheus.io/scrape: "adaptive"
    prometheus.io/path: /metrics
    prometheus.io/port: "9009"
  podLabels: {}

  nodeSelector: {}

  # Uncomment to allow control plane to be scheduled on GPU nodes
  tolerations:
  #   - key: "nvidia.com/gpu"
  #     operator: "Exists"
  #     effect: "NoSchedule"

  # Pod affinity/anti-affinity rules
  # Example: Schedule control plane pods on different nodes
  # affinity:
  #   podAntiAffinity:
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #       - weight: 100
  #         podAffinityTerm:
  #           labelSelector:
  #             matchLabels:
  #               app.kubernetes.io/component: control-plane
  #           topologyKey: kubernetes.io/hostname
  affinity: {}

  # Topology spread constraints for pod distribution
  # Example: Spread control plane pods across availability zones
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: topology.kubernetes.io/zone
  #     whenUnsatisfiable: ScheduleAnyway
  #     labelSelector:
  #       matchLabels:
  #         app.kubernetes.io/component: control-plane
  topologySpreadConstraints: []

  # Pod Disruption Budget for control plane
  # Ensures minimum availability during voluntary disruptions (node drains, upgrades)
  # NOTE: Specify either minAvailable OR maxUnavailable, not both
  pdb:
    enabled: false
    # minAvailable: 1    # Minimum number/percentage of pods that must remain available
    # maxUnavailable: 1  # Maximum number/percentage of pods that can be unavailable

  extraEnvVars: {}

  # Resource limits and requests for the control plane
  # Uncomment and adjust based on your workload requirements
  # resources:
  #   limits:
  #     cpu: 2000m
  #     memory: 2Gi
  #   requests:
  #     cpu: 500m
  #     memory: 1Gi

# Sandboxing service
sandkasten:
  servicePort: 3005
  podAnnotations:
    ingest-adaptive-logs: "true"
  podLabels: {}
  nodeSelector: {}
  tolerations: {}

  # Pod affinity/anti-affinity rules
  # Example: Schedule sandkasten pods on different nodes for high availability
  # affinity:
  #   podAntiAffinity:
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #       - weight: 100
  #         podAffinityTerm:
  #           labelSelector:
  #             matchLabels:
  #               app.kubernetes.io/component: sandkasten
  #           topologyKey: kubernetes.io/hostname
  affinity: {}

  # Topology spread constraints for pod distribution
  # Example: Spread sandkasten pods across availability zones
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: topology.kubernetes.io/zone
  #     whenUnsatisfiable: ScheduleAnyway
  #     labelSelector:
  #       matchLabels:
  #         app.kubernetes.io/component: sandkasten
  topologySpreadConstraints: []

  # Pod Disruption Budget for sandkasten
  # Ensures minimum availability during voluntary disruptions (node drains, upgrades)
  # NOTE: Specify either minAvailable OR maxUnavailable, not both
  pdb:
    enabled: false
    # minAvailable: 1    # Minimum number/percentage of pods that must remain available
    # maxUnavailable: 1  # Maximum number/percentage of pods that can be unavailable

  extraEnvVars: {}
  replicaCount: 2
  image:
    repository: adaptive-repository   # Add the Adaptive Repository you have been granted access to
    tag: latest   # Add the sandkasten image tag
    pullPolicy: Always
  resources:
    limits:
      cpu: 2000m
      memory: 32Gi
    requests:
      cpu: 2000m
      memory: 32Gi

# OpenTelemetry Collector configuration
otelCollector:
  # Set to false to disable the OpenTelemetry Collector
  enabled: true

  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
    tag: "0.143.1"
    pullPolicy: IfNotPresent

  replicaCount: 2

  service:
    type: ClusterIP

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

  # Memory limiter processor settings (in MiB)
  memoryLimitMiB: 400
  memorySpikeLimit: 100

  podAnnotations: {}
  podLabels: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  extraEnvVars: {}

  # Pod Disruption Budget for OpenTelemetry Collector
  # Ensures minimum availability during voluntary disruptions (node drains, upgrades)
  # NOTE: Specify either minAvailable OR maxUnavailable, not both
  pdb:
    enabled: true
    minAvailable: 1
    # maxUnavailable: 1

  # Prometheus scraping configuration
  # Scrapes metrics from pods with annotation: prometheus.io/scrape: "adaptive"
  prometheus:
    scrapeInterval: "15s"

  # Resource attributes added to OTEL_RESOURCE_ATTRIBUTES env var on all pods
  resourceAttributes:
    # Override the environment name (defaults to release name)
    environmentName: ""
    # Additional attributes to append (comma-separated key=value pairs)
    # Example: "service.namespace=production,deployment.region=us-east-1"
    extra: ""

  # Exporters configuration
  # Define where to send telemetry data (traces, metrics, logs)
  # The receiver (otlp HTTP on port 4318) and processors (batch, memory_limiter) are fixed
  exporters: |
    # Debug exporter for troubleshooting (logs to stdout)
    debug:
      verbosity: detailed
    # Example: OTLP exporter to send data to another collector or backend
    # otlp:
    #   endpoint: "your-otlp-endpoint:4317"
    #   tls:
    #     insecure: true
    # Example: Jaeger exporter
    # otlp/jaeger:
    #   endpoint: "jaeger-collector:4317"
    #   tls:
    #     insecure: true

  # Pipelines configuration
  # Define how data flows from receivers through processors to exporters
  # Available receivers: [otlp, prometheus]
  # Available processors: [memory_limiter, batch]
  pipelines: |
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [debug]
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [debug]
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [debug]

mlflow:
  # If set to true, jobs will use MLflow for experiment tracking
  # A separate MLflow tracking server will be deployed
  enabled: true
  # External MLflow configuration
  # Set external.enabled to true to use an existing external MLflow server
  # instead of deploying one within the cluster
  external:
    enabled: false
    # URL of the external MLflow tracking server (e.g., "http://mlflow.example.com:5000")
    url: ""
  imageUri: ghcr.io/mlflow/mlflow:v3.1.1
  image:
    pullPolicy: IfNotPresent

  workers: 4  # Recommended: 2-4 workers per CPU core. With 1 CPU limit, 4 workers is optimal

  # MLflow server configuration
  backendStoreUri: sqlite:///mlflow-storage/mlflow.db
  # Use mlflow-artifacts:/ URI scheme - artifacts sent via HTTP to server, stored server-side
  # This way, multiple training partitions can upload artifacts without relying on shared storage
  # Highly advised to stick to this scheme
  defaultArtifactRoot: mlflow-artifacts:/
  serveArtifacts: true

  # Persistence configuration for MLflow data (database and artifacts)
  persistence:
    enabled: false  # Set to true to use a PersistentVolumeClaim
    storageClass: ""  # Use default storage class if empty
    size: 10Gi
    accessModes:
      - ReadWriteOnce
    mountPath: /mlflow-storage  # Mount path for persistent storage

  # Legacy storage configuration (used when persistence.enabled=false)
  # These volumes/volumeMounts are for non-persistent storage or custom volume configurations
  volumes:
    - name: mlflow-storage
      emptyDir: {}
    # Example for NFS hostPath:
    # - name: mlflow-storage
    #   hostPath:
    #     path: /mnt/nfs/mlflow
    #     type: Directory

  mountPath: ""
  # Ensure mountPath matches the backendStoreUri
  volumeMounts:
    - name: mlflow-storage
      mountPath: /mlflow-storage

  # Resources for MLflow server
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

  # Node selection and tolerations
  nodeSelector: {}
  tolerations: []

  # Pod annotations and labels
  podAnnotations: {}
  podLabels: {}

  # Additional environment variables
  extraEnvVars: {}

# Optional: This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

redis:  # Redis is required and cannot be disabled
  # OPTIONAL: Set to true to deploy an internal Redis instance in the cluster
  # DEFAULT BEHAVIOR: Deploy internal Redis (set install.enabled=false to use external Redis)
  # When install.enabled=true, the internal Redis service will be used instead of external Redis
  # We recommend using an external managed Redis for production
  install:
    enabled: true  # Set to false to use external Redis endpoint

  # Internal Redis configuration (used when install.enabled=true)
  image:
    repository: redis
    tag: "7.4.7-alpine"
    pullPolicy: IfNotPresent

  port: 6379

  # Redis authentication (optional, only used for internal Redis)
  auth:
    username: ""  # Leave empty for no username
    password: ""  # Leave empty for no password

  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 512Mi

  podAnnotations: {}
  podLabels: {}
  extraEnvVars: {}
  nodeSelector: {}
  tolerations: []

  # External Redis configuration (used when install.enabled=false)
  external:
    # URL of the external Redis server (e.g., "redis://redis.example.com:6379" or "redis://user:pass@redis.example.com:6379")
    url: ""  # Required when install.enabled=false

installPostgres:
  # OPTIONAL: Set to true to deploy an internal PostgreSQL database in the cluster
  # DEFAULT BEHAVIOR: Use external PostgreSQL (configured via secrets.db above)
  # When enabled=true, the internal PostgreSQL service will be used instead of external database
  # We strongly recommend using an external managed database for production
  enabled: false

  image:
    repository: postgres
    tag: "17-alpine"
    pullPolicy: IfNotPresent

  port: 5432

  # Database configuration
  database: adaptive
  username: adaptive
  password: ""  # Leave empty to auto-generate, or set a custom password

  # PostgreSQL persistence
  persistence:
    enabled: false
    storageClass: ""  # Use default storage class if empty
    size: 10Gi
    accessModes:
      - ReadWriteOnce

  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

  podAnnotations: {}
  podLabels: {}
  extraEnvVars: {}
  nodeSelector: {}
  tolerations: []
  # Optional init containers to run before the main postgresql container
  # Example:
  # initContainers:
  #   - name: init-db
  #     image: busybox
  #     command: ['sh', '-c', 'echo "Initializing database..."']
  #     volumeMounts:
  #       - name: init-script
  #         mountPath: /scripts
  initContainers: []
  # Extra volumes to be mounted in the pod (available to init containers and main container)
  # Example:
  # extraVolumes:
  #   - name: init-script
  #     configMap:
  #       name: postgres-init-script
  #   - name: backup-storage
  #     persistentVolumeClaim:
  #       claimName: postgres-backup-pvc
  extraVolumes: []
  # Extra volume mounts for the main postgresql container
  # Example:
  # extraVolumeMounts:
  #   - name: init-script
  #     mountPath: /docker-entrypoint-initdb.d
  #     readOnly: true
  extraVolumeMounts: []

# Additional volumes/mount for adaptive stack. They don't need to be defined in most of the cases. For cases where model registry is in external nfs mount.
volumes: []
    # - name: model-registry
    #   persistentVolumeClaim:
    #     claimName: model-registry-pvc

volumeMounts: []
    # - name: model-registry
    #   mountPath: /model-registry

# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
